datalake {
  connect = "jdbc:postgresql://localhost/postgres?user=postgres&password=actio"
  folder = "/home/maurice/temp/fileupload"
}

script {
  settings {
    namespace = "actio.datapipes.task.Term.Functions"
    version = "v2"
    port = 8080
  }
  services = [
    {
      path = "api/fileready"
      post = "p_fileready"
    }
  ]
  tasks {
    unzip_tar_gz {
      type = extract
      dataSource {
        type = zip
        query {
          read {
            path = ${datalake.folder}"/${this.body.ID}.bin"
          }
        }
      }
    }
    read_json_file {
      type = extract
      size = 1
      dataSource {
        type = "file"
        behavior = "json"
        directory = ${datalake.folder}"/"
        query {
          read {
            filenameTemplate = "${name}"
          }
        }
      }
    }
    write_stage_DB {
      type = stage_load
      dataSource {
        type = "sql"
        connect = ${datalake.connect}
        query {
          initialise = "CREATE TABLE IF NOT EXISTS datalake.t_${array(0).parameters.view}(runid uuid, createddate timestamp without time zone DEFAULT now(), configname character varying(100), pipename character varying(100), parameters jsonb, data jsonb)"
          create = "insert into datalake.t_${array(0).parameters.view}(runid,configname,pipename,parameters,data) values ('${array(0).run.id}','${array(0).run.configName}','${array(0).run.pipeName}','${sq(array(0).parameters.toJson())}','${sq(array(0).array.toJson())}')"
        }
      }
    }
    print_display {
      type = dump
      format = "json"
    }
  }
  pipelines {
    p_fileready {
      pipe = "unzip_tar_gz | read_json_file | write_stage_DB"
    }
  }
  startup {
    exec = "p_fileready"
  }
}
